---
sidebar_position: 3
slug: /module3/chapter2-vslam
title: Chapter 2 - Isaac ROS Visual SLAM
description: Complete guide to implementing Isaac ROS Visual SLAM for humanoid robot localization and mapping
---

# Chapter 2: Isaac ROS Visual SLAM

This chapter covers the implementation of Isaac ROS Visual SLAM (VSLAM) for humanoid robot localization and mapping, including stereo vision processing, loop closure, and performance optimization.

## Introduction to Visual SLAM

Visual Simultaneous Localization and Mapping (VSLAM) is a critical component for autonomous humanoid robots. It enables the robot to:
- Localize itself in unknown environments
- Build maps of the surroundings
- Navigate autonomously using visual information
- Maintain accurate position estimates over time

### VSLAM Pipeline
```
Stereo Images → Feature Detection → Motion Estimation → Mapping → Loop Closure → Localization
```

## Isaac ROS VSLAM Architecture

### Core Components
- **Feature Detection**: ORB, FAST, or Harris corner detection
- **Motion Estimation**: Visual odometry and pose estimation
- **Mapping**: 3D landmark creation and management
- **Loop Closure**: Drift correction and map optimization
- **Bundle Adjustment**: Global and local optimization

### System Architecture
```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│  Stereo Camera  │───▶│  VSLAM Pipeline  │───▶│  Localization   │
│  (Left/Right)   │    │                  │    │  & Mapping      │
│                 │    │  - Feature       │    │                 │
│  - Rectification│    │    Detection     │    │  - Pose Graph   │
│  - Calibration  │    │  - Tracking      │    │  - Optimization │
└─────────────────┘    │  - Estimation    │    │  - Output       │
                       │  - Optimization  │    │    Formats      │
                       └──────────────────┘    └─────────────────┘
```

## Installation and Setup

### Prerequisites
- ROS 2 Humble Hawksbill
- NVIDIA GPU with CUDA support
- Isaac ROS packages installed
- Stereo camera calibrated

### Installation Steps
```bash
# Install Isaac ROS VSLAM package
sudo apt update
sudo apt install ros-humble-isaac-ros-visual-slam

# Verify installation
ros2 run isaac_ros_visual_slam visual_slam_node --ros-args --help
```

## Configuration and Parameters

### Basic Configuration
```yaml
# vslam_params.yaml
input:
  # Number of cameras (1 for monocular, 2 for stereo)
  num_cameras: 2

  # Minimum number of images to initialize tracking
  min_num_images: 30

  # Maximum frame rate for processing
  max_frame_rate: 60.0  # Hz

# Feature Detection Parameters
feature_detection:
  # Type of feature detector to use
  feature_detector_type: "ORB"  # Options: ORB, FAST, HARRIS

  # Number of features to detect per image
  num_features: 1000

  # Pyramid scaling factor
  scale_factor: 1.2

  # Number of pyramid levels
  num_levels: 8

  # Edge threshold for feature detection
  edge_threshold: 19

# Motion Estimation Parameters
motion_estimation:
  # Reprojection error threshold for valid matches
  reproj_err_threshold: 3.0  # pixels

  # Minimum number of inliers for valid motion estimation
  min_num_inliers: 15

  # RANSAC confidence for motion estimation
  ransac_confidence: 0.99

# Loop Closure Parameters
loop_closure:
  # Enable loop closure detection
  enable_loop_closure: true

  # Frequency of loop closure checks (Hz)
  loop_closure_frequency: 1.0

  # Minimum score for loop closure detection
  loop_closure_min_score: 0.7

  # Enable pose graph optimizer
  enable_pose_graph_optimizer: true

# Bundle Adjustment Parameters
bundle_adjustment:
  # Enable global bundle adjustment
  enable_global_ba: true

  # Frequency of global bundle adjustment (keyframes)
  global_ba_frequency: 5

  # Enable local bundle adjustment
  enable_local_ba: true

# Keyframe Selection Parameters
keyframe_selection:
  # Minimum translation for keyframe selection (meters)
  min_translation_keyframe_delta: 0.1

  # Minimum rotation for keyframe selection (radians)
  min_rotation_keyframe_delta: 0.1

# GPU Acceleration Parameters
gpu_acceleration:
  # Enable GPU acceleration
  enable_gpu_acceleration: true

  # GPU device ID
  gpu_id: 0

# Output Configuration
output:
  # Enable rectified pose output
  enable_rectified_pose: true

  # Enable debug mode
  enable_debug_mode: false

  # Enable SLAM visualization
  enable_slam_visualization: true

# Frame IDs
frame_ids:
  # Map frame ID
  map_frame: "map"

  # Odometry frame ID
  odom_frame: "odom"

  # Base frame ID (robot base)
  base_frame: "base_link"

  # Camera frame ID
  camera_frame: "camera_link"
```

## Visual Odometry

### Stereo Visual Odometry
Stereo VSLAM uses two synchronized cameras to estimate depth and motion:

```python
# Example: Stereo visual odometry implementation
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from sensor_msgs.msg import CameraInfo
from geometry_msgs.msg import PoseStamped
from cv_bridge import CvBridge
import numpy as np
import cv2

class StereoVisualOdometry(Node):
    def __init__(self):
        super().__init__('stereo_vo')

        # Initialize CV bridge
        self.bridge = CvBridge()

        # Initialize subscribers
        self.left_sub = self.create_subscription(
            Image, '/camera/left/image_rect', self.left_callback, 10)
        self.right_sub = self.create_subscription(
            Image, '/camera/right/image_rect', self.right_callback, 10)

        # Initialize publishers
        self.odom_pub = self.create_publisher(PoseStamped, '/vslam/odometry', 10)

        # Initialize stereo matcher
        self.stereo = cv2.StereoSGBM_create(
            minDisparity=0,
            numDisparities=64,
            blockSize=5,
            P1=8 * 3 * 5**2,
            P2=32 * 3 * 5**2,
            disp12MaxDiff=1,
            uniquenessRatio=15,
            speckleWindowSize=0,
            speckleRange=2,
            preFilterCap=63,
            mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY
        )

        # Initialize tracking variables
        self.prev_left = None
        self.prev_right = None
        self.prev_pose = np.eye(4)
        self.current_pose = np.eye(4)

    def left_callback(self, msg):
        self.left_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')

    def right_callback(self, msg):
        self.right_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='mono8')

        if hasattr(self, 'left_image'):
            self.process_stereo_frame()

    def process_stereo_frame(self):
        # Process stereo pair for depth and motion
        if self.prev_left is not None:
            # Feature matching between current and previous frames
            features_prev = self.extract_features(self.prev_left)
            features_curr = self.extract_features(self.left_image)

            # Match features
            matches = self.match_features(features_prev, features_curr)

            # Estimate motion
            motion = self.estimate_motion(matches)

            # Update pose
            self.current_pose = self.current_pose @ motion

            # Publish odometry
            self.publish_odometry()

        # Update previous frames
        self.prev_left = self.left_image.copy()
        self.prev_right = self.right_image.copy()

    def extract_features(self, image):
        # Extract ORB features
        orb = cv2.ORB_create(nfeatures=1000)
        kp, des = orb.detectAndCompute(image, None)
        return kp, des

    def match_features(self, features1, features2):
        # Match features using FLANN
        kp1, des1 = features1
        kp2, des2 = features2

        if des1 is not None and des2 is not None:
            bf = cv2.BFMatcher()
            matches = bf.knnMatch(des1, des2, k=2)

            # Apply Lowe's ratio test
            good_matches = []
            for m, n in matches:
                if m.distance < 0.75 * n.distance:
                    good_matches.append(m)

            return good_matches
        return []

    def estimate_motion(self, matches):
        # Estimate motion using matched features
        if len(matches) >= 10:
            src_pts = np.float32([self.prev_left.kp[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
            dst_pts = np.float32([self.left_image.kp[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

            # Estimate essential matrix
            E, mask = cv2.findEssentialMat(src_pts, dst_pts, self.camera_matrix,
                                         threshold=1, prob=0.999)

            if E is not None:
                # Recover pose
                _, R, t, _ = cv2.recoverPose(E, src_pts, dst_pts, self.camera_matrix)

                # Create transformation matrix
                T = np.eye(4)
                T[:3, :3] = R
                T[:3, 3] = t.ravel()

                return T
        return np.eye(4)

    def publish_odometry(self):
        # Publish odometry message
        odom_msg = PoseStamped()
        odom_msg.header.stamp = self.get_clock().now().to_msg()
        odom_msg.header.frame_id = 'map'

        # Convert pose to message
        pos = self.current_pose[:3, 3]
        odom_msg.pose.position.x = pos[0]
        odom_msg.pose.position.y = pos[1]
        odom_msg.pose.position.z = pos[2]

        # Convert rotation matrix to quaternion
        quat = self.rotation_matrix_to_quaternion(self.current_pose[:3, :3])
        odom_msg.pose.orientation.x = quat[0]
        odom_msg.pose.orientation.y = quat[1]
        odom_msg.pose.orientation.z = quat[2]
        odom_msg.pose.orientation.w = quat[3]

        self.odom_pub.publish(odom_msg)

    def rotation_matrix_to_quaternion(self, R):
        # Convert rotation matrix to quaternion
        trace = np.trace(R)
        if trace > 0:
            s = np.sqrt(trace + 1.0) * 2
            w = 0.25 * s
            x = (R[2, 1] - R[1, 2]) / s
            y = (R[0, 2] - R[2, 0]) / s
            z = (R[1, 0] - R[0, 1]) / s
        else:
            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:
                s = np.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2]) * 2
                w = (R[2, 1] - R[1, 2]) / s
                x = 0.25 * s
                y = (R[0, 1] + R[1, 0]) / s
                z = (R[0, 2] + R[2, 0]) / s
            elif R[1, 1] > R[2, 2]:
                s = np.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2]) * 2
                w = (R[0, 2] - R[2, 0]) / s
                x = (R[0, 1] + R[1, 0]) / s
                y = 0.25 * s
                z = (R[1, 2] + R[2, 1]) / s
            else:
                s = np.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1]) * 2
                w = (R[1, 0] - R[0, 1]) / s
                x = (R[0, 2] + R[2, 0]) / s
                y = (R[1, 2] + R[2, 1]) / s
                z = 0.25 * s

        return [x, y, z, w]
```

## Loop Closure and Drift Correction

### Loop Detection
Loop closure is essential for correcting accumulated drift:

```python
# Example: Loop closure detection
import numpy as np
from sklearn.cluster import DBSCAN
from scipy.spatial.distance import pdist, squareform

class LoopClosureDetector:
    def __init__(self, min_loop_score=0.7, max_distance=5.0):
        self.min_loop_score = min_loop_score
        self.max_distance = max_distance
        self.keyframes = []
        self.poses = []
        self.descriptors = []

    def add_keyframe(self, pose, descriptor):
        """Add a keyframe to the database"""
        self.poses.append(pose)
        self.descriptors.append(descriptor)

        # Check for potential loops
        if len(self.poses) > 10:  # Need minimum keyframes for loop detection
            self.check_for_loop(len(self.poses) - 1)

    def check_for_loop(self, current_idx):
        """Check if current keyframe matches previous keyframes"""
        current_pose = self.poses[current_idx]
        current_desc = self.descriptors[current_idx]

        for i, (pose, desc) in enumerate(zip(self.poses[:-1], self.descriptors[:-1])):
            # Check spatial proximity
            dist = np.linalg.norm(current_pose[:3, 3] - pose[:3, 3])

            if dist < self.max_distance:
                # Compute similarity score
                similarity = self.compute_similarity(current_desc, desc)

                if similarity > self.min_loop_score:
                    # Potential loop closure detected
                    self.handle_loop_closure(current_idx, i, similarity)

    def compute_similarity(self, desc1, desc2):
        """Compute similarity between two descriptors"""
        if desc1 is None or desc2 is None or len(desc1) == 0 or len(desc2) == 0:
            return 0.0

        # Use FLANN matcher to find matches
        bf = cv2.BFMatcher()
        matches = bf.knnMatch(desc1, desc2, k=2)

        # Calculate similarity based on good matches
        good_matches = []
        for m, n in matches:
            if m.distance < 0.75 * n.distance:
                good_matches.append(m)

        if len(matches) > 0:
            return len(good_matches) / len(matches)
        return 0.0

    def handle_loop_closure(self, current_idx, match_idx, score):
        """Handle detected loop closure"""
        print(f"Loop closure detected: {current_idx} -> {match_idx}, score: {score}")

        # Trigger pose graph optimization
        self.optimize_pose_graph(current_idx, match_idx)
```

## Map Management and Optimization

### Keyframe Management
```python
# Example: Keyframe selection and management
class KeyframeManager:
    def __init__(self, min_translation=0.1, min_rotation=0.1):
        self.min_translation = min_translation
        self.min_rotation = min_rotation
        self.keyframes = []
        self.poses = []
        self.landmarks = []

    def should_add_keyframe(self, current_pose, prev_pose=None):
        """Determine if current frame should be a keyframe"""
        if prev_pose is None and len(self.poses) > 0:
            prev_pose = self.poses[-1]
        elif prev_pose is None:
            return True  # First frame is always a keyframe

        # Calculate translation
        translation = np.linalg.norm(
            current_pose[:3, 3] - prev_pose[:3, 3]
        )

        # Calculate rotation
        R_rel = current_pose[:3, :3] @ prev_pose[:3, :3].T
        trace = np.trace(R_rel)
        rotation = np.arccos(np.clip((trace - 1) / 2, -1, 1))

        # Check if translation or rotation exceeds thresholds
        return translation > self.min_translation or rotation > self.min_rotation

    def add_keyframe(self, image, pose, features):
        """Add a new keyframe"""
        if self.should_add_keyframe(pose):
            keyframe = {
                'image': image,
                'pose': pose,
                'features': features,
                'timestamp': rclpy.Time()
            }
            self.keyframes.append(keyframe)
            self.poses.append(pose)
            return True
        return False
```

## Performance Optimization

### Jetson-Specific Optimization
```yaml
# vslam_params_jetson.yaml - Optimized for Jetson Orin
input:
  num_cameras: 2
  min_num_images: 20  # Reduced for Jetson
  max_frame_rate: 25.0  # Lower frame rate for Jetson

feature_detection:
  feature_detector_type: "ORB"
  num_features: 800  # Reduced for Jetson
  scale_factor: 1.2
  num_levels: 6  # Reduced pyramid levels
  edge_threshold: 19
  patch_size: 19  # Smaller patch for speed

motion_estimation:
  reproj_err_threshold: 3.0
  min_num_inliers: 12  # Reduced for Jetson
  ransac_confidence: 0.99
  ransac_max_iter: 500  # Reduced iterations

loop_closure:
  enable_loop_closure: true
  loop_closure_frequency: 0.5  # Less frequent on Jetson
  loop_closure_min_score: 0.6  # Lower threshold
  enable_pose_graph_optimizer: true
  pose_graph_optimizer_frequency: 10  # Every 10 seconds

bundle_adjustment:
  enable_global_ba: false  # Disabled for Jetson performance
  enable_local_ba: true
  local_ba_max_num_iterations: 10  # Reduced iterations

keyframe_selection:
  min_translation_keyframe_delta: 0.15  # Increased for Jetson
  min_rotation_keyframe_delta: 0.15
  min_num_features_for_tracking: 25

gpu_acceleration:
  enable_gpu_acceleration: true
  gpu_id: 0
  cuda_streams: 2  # Reduced for Jetson memory
  enable_memory_pool: true
  memory_pool_size: 67108864  # 64 MB for Jetson

output:
  enable_rectified_pose: true
  enable_debug_mode: false
  enable_slam_visualization: true
  enable_landmarks_view: true
  enable_observations_view: false  # Disabled for performance
```

## Launch Files

### Simulation Launch
```python
# vslam_sim_launch.py
from launch import LaunchDescription
from launch_ros.actions import Node
from launch.actions import DeclareLaunchArgument
from launch.substitutions import LaunchConfiguration
from launch.conditions import IfCondition
from launch.substitutions import PathJoinSubstitution
from launch_ros.substitutions import FindPackageShare

def generate_launch_description():
    # Launch configuration
    enable_loop_closure = LaunchConfiguration('enable_loop_closure')
    enable_debug_mode = LaunchConfiguration('enable_debug_mode')
    enable_mapping = LaunchConfiguration('enable_mapping')

    # Declare launch arguments
    declare_enable_loop_closure_cmd = DeclareLaunchArgument(
        'enable_loop_closure',
        default_value='true',
        description='Enable loop closure detection for drift correction'
    )

    declare_enable_debug_mode_cmd = DeclareLaunchArgument(
        'enable_debug_mode',
        default_value='false',
        description='Enable debug mode with additional visualizations'
    )

    declare_enable_mapping_cmd = DeclareLaunchArgument(
        'enable_mapping',
        default_value='true',
        description='Enable 3D map building'
    )

    # Isaac ROS Visual SLAM node
    visual_slam_node = Node(
        package='isaac_ros_visual_slam',
        executable='isaac_ros_visual_slam_node',
        name='visual_slam',
        parameters=[{
            # Enable rectified pose output
            'enable_rectified_pose': True,

            # Enable debug mode for visualization
            'enable_debug_mode': enable_debug_mode,

            # Enable SLAM visualization
            'enable_slam_visualization': True,

            # Enable landmarks view for debugging
            'enable_landmarks_view': True,

            # Enable observations view (disable for performance)
            'enable_observations_view': False,

            # Frame IDs for TF tree
            'map_frame': 'map',
            'odom_frame': 'odom',
            'base_frame': 'base_link',
            'camera_frame': 'camera_link',

            # Number of cameras (2 for stereo VSLAM)
            'num_cameras': 2,

            # Minimum number of images for initialization
            'min_num_images': 30,

            # Maximum frame rate for processing
            'max_frame_rate': 60.0,

            # Feature detector parameters
            'feature_detector_type': 'ORB',
            'num_features': 1000,
            'scale_factor': 1.2,
            'num_levels': 8,
            'edge_threshold': 19,
            'wta_k': 2,
            'score_type': 0,  # HARRIS_SCORE
            'patch_size': 31,
            'fast_threshold': 20,

            # Motion estimation parameters
            'reproj_err_threshold': 3.0,
            'min_num_inliers': 15,
            'ransac_confidence': 0.99,
            'ransac_max_iter': 1000,

            # Loop closure parameters
            'enable_loop_closure': enable_loop_closure,
            'loop_closure_frequency': 1.0,  # Hz
            'loop_closure_min_score': 0.7,
            'enable_pose_graph_optimizer': True,
            'pose_graph_optimizer_frequency': 1.0,  # Hz

            # Bundle adjustment parameters
            'enable_global_ba': True,
            'global_ba_frequency': 5,  # Run every 5 keyframes
            'global_ba_max_num_iterations': 100,
            'enable_local_ba': True,
            'local_ba_max_num_iterations': 20,

            # Keyframe selection parameters
            'min_translation_keyframe_delta': 0.1,
            'min_rotation_keyframe_delta': 0.1,
            'min_num_features_for_tracking': 30,

            # GPU acceleration parameters
            'gpu_id': 0,
            'enable_gpu_acceleration': True,
        }],
        remappings=[
            # Input remappings for stereo camera
            ('/visual_slam/image_0', '/camera/left/image_rect'),
            ('/visual_slam/camera_info_0', '/camera/left/camera_info'),
            ('/visual_slam/image_1', '/camera/right/image_rect'),
            ('/visual_slam/camera_info_1', '/camera/right/camera_info'),

            # Output remappings
            ('/visual_slam/tracking/odometry', '/vslam/odometry'),
            ('/visual_slam/tracking/vo_pose', '/vslam/vo_pose'),
            ('/visual_slam/tracking/vo_pose_covariance', '/vslam/vo_pose_covariance'),

            # Visualization remappings
            ('/visual_slam/vis/landmarks_cloud', '/vslam/landmarks'),
            ('/visual_slam/vis/loop_closure_cloud', '/vslam/loop_closure'),
            ('/visual_slam/vis/pose_graph_nodes', '/vslam/pose_graph_nodes'),
            ('/visual_slam/vis/pose_graph_edges', '/vslam/pose_graph_edges'),
            ('/visual_slam/vis/keyframes', '/vslam/keyframes'),
        ]
    )

    # Optional: Add stereo image processing node for rectification
    stereo_rectify_node = Node(
        package='stereo_image_proc',
        executable='stereo_image_proc',
        name='stereo_rectify',
        parameters=[
            {'approximate_sync': True},
            {'queue_size': 10}
        ],
        remappings=[
            ('left/image_raw', '/camera/left/image_raw'),
            ('left/camera_info', '/camera/left/camera_info'),
            ('right/image_raw', '/camera/right/image_raw'),
            ('right/camera_info', '/camera/right/camera_info'),
            ('left/image_rect', '/camera/left/image_rect'),
            ('right/image_rect', '/camera/right/image_rect'),
        ]
    )

    # Optional: Add RViz2 for visualization
    rviz_node = Node(
        package='rviz2',
        executable='rviz2',
        name='rviz2',
        arguments=['-d', PathJoinSubstitution([
            FindPackageShare('isaac_ros_visual_slam'),
            'rviz', 'visual_slam.rviz'
        ])],
        condition=IfCondition(LaunchConfiguration('enable_rviz', default='false'))
    )

    return LaunchDescription([
        declare_enable_loop_closure_cmd,
        declare_enable_debug_mode_cmd,
        declare_enable_mapping_cmd,

        visual_slam_node,
        stereo_rectify_node,
        rviz_node,
    ])
```

## Debugging and Troubleshooting

### Common Issues and Solutions

#### 1. Poor Tracking Performance
- **Cause**: Insufficient features, motion blur, or poor lighting
- **Solution**: Increase feature count, improve lighting, reduce motion speed

#### 2. Drift Accumulation
- **Cause**: Infrequent loop closure or poor loop detection
- **Solution**: Enable loop closure, adjust detection parameters

#### 3. Performance Issues
- **Cause**: High computational load on target hardware
- **Solution**: Use Jetson-optimized parameters, reduce feature count

#### 4. Calibration Problems
- **Cause**: Incorrect camera calibration
- **Solution**: Recalibrate stereo cameras using standard patterns

### Debugging Tools
```bash
# Monitor VSLAM topics
ros2 topic echo /vslam/odometry
ros2 topic echo /vslam/landmarks
ros2 topic hz /camera/left/image_rect

# Check TF tree
ros2 run tf2_tools view_frames

# Monitor performance
ros2 run isaac_ros_visual_slam visual_slam_performance_monitor
```

## Exercises

### Exercise 1: Basic VSLAM Setup
1. Set up stereo cameras with proper calibration
2. Launch Isaac ROS VSLAM with default parameters
3. Verify odometry output and landmark visualization
4. Test in a simple environment

### Exercise 2: Parameter Tuning
1. Experiment with different feature detection parameters
2. Adjust motion estimation thresholds
3. Test loop closure sensitivity
4. Measure performance impact of each parameter

### Exercise 3: Performance Optimization
1. Profile VSLAM performance on your hardware
2. Apply Jetson-optimized parameters
3. Measure frame rate and accuracy trade-offs
4. Validate results in real-world conditions

## Best Practices

### 1. Environment Considerations
- Use textured environments for better feature detection
- Avoid repetitive patterns that confuse tracking
- Ensure adequate lighting conditions

### 2. Hardware Optimization
- Use synchronized stereo cameras
- Ensure stable mounting of cameras
- Calibrate cameras regularly

### 3. Parameter Selection
- Start with default parameters and tune gradually
- Monitor both accuracy and performance
- Validate results in target environment

## Summary

Isaac ROS VSLAM provides a powerful solution for humanoid robot localization and mapping. By properly configuring the system and optimizing for your hardware, you can achieve robust visual SLAM performance for autonomous navigation.

In the next chapter, we'll explore Isaac ROS Navigation 2 for path planning and autonomous movement capabilities.